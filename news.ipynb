{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the entire file content\n",
    "with open(\"/Users/aaryas127/Documents/GitHub/stock_predictor/api_key.txt\", 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n",
    "\n",
    "# Reading the file line by line\n",
    "with open(\"/Users/aaryas127/Documents/GitHub/stock_predictor/api_key.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "  base_url=\"https://integrate.api.nvidia.com/v1\",  # You can change this to OpenAI's URL if needed\n",
    "  api_key=str(content)\n",
    ")\n",
    "\n",
    "# Define the prompt, with the variable 'text' inserted\n",
    "prompt = f\"\"\"\n",
    "How is the bell stock performing?\n",
    "\"\"\"\n",
    "\n",
    "# Request completion from OpenAI's model\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"meta/llama-3.2-3b-instruct\",  # You can switch this model to one compatible with OpenAI if needed\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "# Collect and print the response as it's streamed\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        output = chunk.choices[0].delta.content\n",
    "        print(output, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
